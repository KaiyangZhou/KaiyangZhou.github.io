<!DOCTYPE html>
<html>

<head>
    <title>Kaiyang Zhou's Home Page</title>

    <link rel="stylesheet" href="style.css">

    <meta charset="UTF-8">
    <meta name="description" content="Kaiyang Zhou's Home Page">
    <meta name="keywords" content="Kaiyang Zhou">
    <meta name="author" content="Kaiyang Zhou">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>

<body>

<div id="contents">

<div class="profile-table">
  <img class="profile-img" src="images/ky-bu.jpg" width="180">
  <div class="profile-text">
    <h2> Kaiyang Zhou </h2>
    <p>
      <i>
        Assistant Professor <br>
        <a href="https://www.comp.hkbu.edu.hk/">Department of Computer Science</a> <br>
        <a href="https://www.hkbu.edu.hk/">Hong Kong Baptist University</a> <br>
        Email: kyzhou [at] hkbu.edu.hk
      </i>
    </p>
    <p>
      <a href="https://scholar.google.com/citations?user=gRIejugAAAAJ">Google Scholar</a> |
      <a href="https://github.com/KaiyangZhou">Github</a> |
      <a href="https://twitter.com/kaiyangzhou">Twitter</a>
    </p>
  </div>
</div>

<hr noshade>

<p>
Prof. Kaiyang Zhou is an Assistant Professor in the Department of Computer Science at Hong Kong Baptist University. His research interests include machine learning, computer vision, multimodality, generalization, and efficiency. His research has been published in leading conference venues, including CVPR, ICCV, ECCV, NeurIPS, ICLR, ICML, and AAAI, as well as in top journals like IEEE TPAMI, IEEE TIP, and IJCV. His work has been cited over 18,000 times. He is currently an associate editor for the International Journal of Computer Vision and regularly serves as an area chair for prestigious conferences such as CVPR, ECCV, NeurIPS, ICML, and ICLR. Before joining HKBU, he was a postdoc at Nanyang Technological University, Singapore, working with Prof. <a href="https://liuziwei7.github.io/">Ziwei Liu</a> and Prof. <a href="https://www.mmlab-ntu.com/person/ccloy/">Chen Change Loy</a>. He received his PhD in Computer Science from the University of Surrey, UK, under the supervision of Prof. <a href="https://www.surrey.ac.uk/people/tao-xiang">Tao Xiang</a>.
</p>

<p>
Quick links:
<a href="team.html">Team</a> |
<a href="https://www.maifoundations.com/" target="_blank">Research</a> |
<a href="https://scholar.google.com/citations?hl=en&user=gRIejugAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Publications</a> |
<a href="https://github.com/maifoundations" target="_blank">Software</a> |
<a href="https://huggingface.co/maifoundations" target="_blank">Models and Datasets</a>
</p>

<p><i>
To prospective students: We are working on multimodal AI (VLMs, LLMs, reasoning, agents, GenAI). If you are interested in joining us as PhD/MPhil/RA/intern, please drop me an email with your CV. Due to the high volume of inquiries, only shortlisted candidates will be contacted.
</i></p>

<h3> News </h3>
<ul>
    <li> [2025-11] Invited to serve as area chair of ICML 2026. </li>
    <li> [2025-09] Our edited book <a href="https://link.springer.com/book/10.1007/978-3-031-94969-2">Large Vision-Language Models</a> is online. </li>
    <li> [2025-08] Invited to serve as area chair of CVPR 2026 and ICLR 2026. </li>
    <li> [2025-07] Invited to serve as area chair of AAAI 2026. </li>
    <!-- <details>
    <summary><i>Older News & Activities</i></summary>
    <li> [2025-01] Gave a talk at <a href="https://www.comp.hkbu.edu.hk/wsb2025/">IAPR/IEEE Winter School on Biometrics 2025</a> (<a href="https://drive.google.com/file/d/1FHn2hMmAOXUg1Ok7li9REUZh4IKXblzM/view?usp=sharing">slides</a>). </li>
    <li> [2024-12] Invited to serve as ICML 2025 area chair. </li>
    <li> [2024-10] <a href="https://link.springer.com/journal/11263/updates/27704554">Call for Papers</a>: IJCV Special Issue on Visual Domain Generalization in Real-World Applications. </li>
    <li> [2024-09] Invited to serve as CVPR 2025 area chair. </li>
    <li> [2024-08] Invited to serve as ICLR 2025 area chair. </li>
    <li> [2024-06] Invited to serve as AAAI 2025 senior program committee. </li>
    <li> [2024-06] We're organizing a workshop on <a href="https://prompting-in-vision.github.io/index_cvpr24.html">Prompting in Vision</a> at CVPR 2024. </li>
    <li> [2024-05] Invited to serve as BMVC 2024 area chair. </li>
    <li> [2024-04] <a href="https://green-fomo.github.io/ECCV2024/call.html">Call for Papers</a>: ECCV 2024 Workshop on Green Foundation Models. </li>
    <li> [2024-04] Invited to serve as NeurIPS 2024 area chair. </li>
    <li> [2024-02] <a href="https://prompting-in-vision.github.io/index_cvpr24.html">Call for Papers</a>: CVPR 2024 Workshop on Prompting in Vision. </li>
    <li> [2023-12] Invited to serve as ECCV 2024 area chair. </li>
    <li> [2023-10] Gave a talk at <a href="http://valser.org/article-697-1.html">VALSE</a>. </li>
    <li> [2023-08] Gave a talk at <a href="https://icaigc.org/workshops.html">AIGC-2023 Workshop on Trustworthy Foundation Models under Imperfect Data</a>. </li>
    <li> [2023-08] Gave a talk at <a href="https://www.cair-cas.org.hk/article/ijcai-medical-large-models">IJCAI-2023 Symposium Session on Medical Large Models</a>. </li>
    <li> [2023-08] Invited to serve as CVPR 2024 area chair. </li>
    <li> [2023-08] Invited to join the editorial board of International Journal of Computer Vision as associate editor. </li>
    <li> [2023-07] Gave a talk at the University of Tokyo (<a href="https://prompting-in-vision.github.io/slides/cvpr23/lecture-1a.pdf">slides</a> & <a href="https://youtu.be/fIoeCI3gH94">video</a>). </li>
    <li> [2023-06] We're organizing a tutorial on <a href="https://prompting-in-vision.github.io/">Prompting in Vision</a> at CVPR 2023. </li>
    <li> [2023-01] <a href="https://domaingen.github.io/">Call for Papers</a>: ICLR 2023 workshop on what do we need for successful domain generalization? </li>
    <li> [2022-09] <a href="assets/cfp_ijcv_lvms.html">Call for Papers</a>: IJCV Special Issue on The Promises and Dangers of Large Vision Models. </li>
  </details> -->
</ul>

<h3> Latest Research </h3>
<ul>
    <li> <a href="https://www.maifoundations.com/blog/humblebench/">HumbleBench: Measuring Epistemic Humility in Multimodal LLMs</a> </li>
    <li> <a href="https://www.maifoundations.com/blog/gcot/">Grounded Chain-of-Thought Makes Multimodal LLMs More Data-Efficient</a> </li>
    <li> <a href="https://www.maifoundations.com/blog/qzo/">Fine-Tuning 13B LLM or Stable Diffusion 3.5 Large Within a Single 24GB GPU</a> </li>
    <li> <a href="https://www.maifoundations.com/blog/indexmark/">Watermarking Autoregressive Image Generation Models</a> </li>
    <li> <a href="https://www.maifoundations.com/blog/visionary-r1/">Mitigating Shortcuts in Visual Reasoning with Reinforcement Learning</a> </li>
</ul>

<h3> Teaching </h3>
<ul>
  <li>
    <b>Hong Kong Baptist University</b> <br>
    <i>COMP7040: Advanced Topics in Computer Vision and Pattern Recognition</i> <br>
    <i>COMP3076: AI and Generative Arts</i> <br>
    <i>COMP7065: Innovative Laboratory</i>
  </li>
  <li>
    <b>Nanyang Technological University</b> <br>
    <i>AI6126 Guest Lecture: Open-World Visual Recognition</i> <br>
    <i>OpenMMLab Workshop: Object Detection</i>
  </li>
  <li>
    <b>Queen Mary University of London</b> <br>
    <i>ECS797: Machine Learning for Visual Data Analytics</i> <br>
    <i>ECS708: Machine Learning</i>
  </li>
</ul>

<h3> Academic Services </h3>
<ul>
<li> Associate Editor: International Journal of Computer Vision (IJCV) </li>
<li> Guest Editor: IJCV Special Issue on <a href="assets/cfp_ijcv_lvms.html">The Promises and Dangers of Large Vision Models</a> </li>
<li> Area Chair: ICML, ICLR, NeurIPS, CVPR, ECCV, BMVC, AAAI </li>
<li> Organizing Committee: <a href="https://theaitalks.org">The AI Talks</a>, <a href="https://prompting-in-vision.github.io/index_cvpr23.html">CVPR'23 Tutorial on Prompting</a>, <a href="https://domaingen.github.io">ICLR'23 Workshop on DG</a>, <a href="https://prompting-in-vision.github.io/index_cvpr24.html">CVPR'24 Workshop on Prompting</a>, <a href="https://green-fomo.github.io/ECCV2024/index.html">ECCV'24 Workshop on Green Foundation Models</a>, etc. </li>
</ul>

</div>
</body>
</html>

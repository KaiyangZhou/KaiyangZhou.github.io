<!DOCTYPE html>
<html>

<head>
    <title>Kaiyang Zhou's Home Page</title>

    <link rel="stylesheet" href="style.css">

    <meta charset="UTF-8">
    <meta name="description" content="Kaiyang Zhou's Home Page">
    <meta name="keywords" content="Kaiyang Zhou">
    <meta name="author" content="Kaiyang Zhou">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>

<body>

<div id="contents">

<div class="profile-table">
  <img class="profile-img" src="images/ky-bu.jpg" width="180">
  <div class="profile-text">
    <h2> Kaiyang Zhou </h2>
    <p>
      <i>
        Assistant Professor <br>
        Department of Computer Science <br>
        Hong Kong Baptist University <br>
        Email: kyzhou [at] hkbu.edu.hk
      </i>
    </p>
    <p>
      <a href="pub.html">Publications</a> |
      <a href="https://scholar.google.com/citations?user=gRIejugAAAAJ">Google Scholar</a> |
      <a href="https://github.com/KaiyangZhou">Github</a> |
      <a href="https://twitter.com/kaiyangzhou">Twitter</a>
    </p>
  </div>
</div>

<hr noshade>

<p>
Dr. Kaiyang Zhou is an Assistant Professor at the <a href="https://www.comp.hkbu.edu.hk/">Department of Computer Science, Hong Kong Baptist University</a>. His research interests lie at the intersection of computer vision and machine learning. He is an associate editor of the International Journal of Computer Vision (IJCV) and has served as area chair or senior program committee member for CVPR, ECCV, AAAI, and BMVC. Prior to joining HKBU, he was a postdoc at Nanyang Technological University, Singapore, working with Prof. <a href="https://liuziwei7.github.io/">Ziwei Liu</a> and Prof. <a href="https://www.mmlab-ntu.com/person/ccloy/">Chen Change Loy</a>. He received his PhD in computer science from the University of Surrey, UK, under the supervision of Prof. <a href="http://personal.ee.surrey.ac.uk/Personal/T.Xiang/index.html">Tao Xiang</a>.
</p>

<h3> Research interests </h3>

<p>
<ul>
    <li> Foundation models: large vision/language models, multimodal models, prompting methods </li>
    <li> Computer vision: visual perception, image generation, video understanding </li>
    <li> Machine learning: domain generalization, domain adaptation, transfer learning </li>
</ul>
</p>

<h3> News </h3>

<ul>
    <li> [2024-02] <a href="https://domaingen.github.io/">Call for Papers</a>: CVPR 2024 Workshop on Prompting in Vision. </li>
    <li> [2023-12] Invited to serve as ECCV 2024 area chair. </li>
    <li> [2023-10] Gave a talk at <a href="http://valser.org/article-697-1.html">VALSE</a>. </li>
    <li> [2023-08] Gave a talk at <a href="https://icaigc.org/workshops.html">AIGC-2023 Workshop on Trustworthy Foundation Models under Imperfect Data</a>. </li>
    <li> [2023-08] Gave a talk at <a href="https://www.cair-cas.org.hk/article/ijcai-medical-large-models">IJCAI-2023 Symposium Session on Medical Large Models</a>. </li>
    <li> [2023-08] Invited to serve as CVPR 2024 area chair. </li>
    <li> [2023-08] Invited to join the editorial board of International Journal of Computer Vision as associate editor. </li>
    <li> [2023-07] Gave a talk at the University of Tokyo (<a href="https://prompting-in-vision.github.io/slides/cvpr23/lecture-1a.pdf">slides</a> & <a href="https://youtu.be/fIoeCI3gH94">video</a>). </li>
    <li> [2023-06] We're organizing a tutorial on <a href="https://prompting-in-vision.github.io/">Prompting in Vision</a> at CVPR 2023. </li>
    <li> [2023-01] <a href="https://domaingen.github.io/">Call for Papers</a>: ICLR 2023 workshop on what do we need for successful domain generalization? </li>
    <li> [2022-09] <a href="assets/cfp_ijcv_lvms.html">Call for Papers</a>: IJCV Special Issue on The Promises and Dangers of Large Vision Models. </li>
</ul>

<h3> Professional services </h3>

<ul>
<li> Associate Editor: International Journal of Computer Vision (IJCV) </li>
<li> Guest Editor: IJCV Special Issue on <a href="assets/cfp_ijcv_lvms.html">The Promises and Dangers of Large Vision Models</a> </li>
<li> Area Chair / Senior Program Committee: ECCV 2024, CVPR 2024, BMVC 2022, AAAI 2023-24 </li>
<li> Organizing Committee: <a href="https://prompting-in-vision.github.io/">CVPR 2023 Prompting Tutorial</a>, <a href="https://domaingen.github.io/">ICLR 2023 DG Workshop</a>, <a href="https://theaitalks.org">The AI Talks</a> </li>
<li> Reviewer: TPAMI, IJCV, ICLR, NeurIPS, ICML, AAAI, CVPR, ICCV, ECCV, etc. </li>
</ul>

<h3> Teaching </h3>

<ul>
  <li>
    Instructor, Hong Kong Baptist University <br>
    <i>COMP7065: Innovative Laboratory</i> <br>
    <i>ITS 7010: ITS Doctoral Research Training I</i>
  </li>
  <li>
    Lecturer, Nanyang Technological University <br>
    <i>AI6126 Guest Lecture: Open-World Visual Recognition</i> <br>
    <i>OpenMMLab Workshop: Object Detection</i>
  </li>
  <li>
    Teaching Assistant, Queen Mary University of London <br>
    <i>ECS797: Machine Learning for Visual Data Analytics</i> <br>
    <i>ECS708: Machine Learning</i>
  </li>
</ul>

<h3> Software and datasets </h3>

<ul>
<li> <a href="https://github.com/KaiyangZhou/deep-person-reid">Torchreid</a>: A codebase for person re-identification (including <a href="https://kaiyangzhou.github.io/deep-person-reid/">documentation</a> and <a href="https://kaiyangzhou.github.io/deep-person-reid/MODEL_ZOO">model zoo</a>). </li>
<li> <a href="https://github.com/KaiyangZhou/Dassl.pytorch">Dassl</a>: A multifunctional codebase for domain generalization, domain adaptation and semi-supervised learning. </li>
<li> <a href="https://github.com/KaiyangZhou/CoOp">CoOp</a>: A codebase for developing adaptation methods (e.g., prompt learning) for large-scale vision-language models. </li>
<li> <a href="https://github.com/Jingkang50/OpenOOD">OpenOOD</a>: A codebase and benchmark for out-of-distribution detection. </li>
<li> <a href="http://psgdataset.org/">PSG</a>: A dataset for panoptic scene graph generation. (Codebase: <a href="https://github.com/Jingkang50/OpenPSG/">OpenPSG</a>.) </li>
</ul>

</div>

</body>
</html>

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Kaiyang Zhou | Hong Kong Baptist University</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Crimson+Pro:ital,wght@0,400;0,500;0,600;1,400&family=Source+Sans+3:wght@400;500;600&display=swap" rel="stylesheet">
</head>
<body>
    <nav class="nav">
        <div class="nav-container">
            <a href="#about" class="nav-link">About</a>
            <a href="#news" class="nav-link">News</a>
            <a href="#research" class="nav-link">Research</a>
            <a href="#team" class="nav-link">Team</a>
            <a href="#teaching" class="nav-link">Teaching</a>
            <a href="#services" class="nav-link">Services</a>
            <a href="#awards" class="nav-link">Awards</a>
        </div>
    </nav>

    <main class="container">
        <!-- About Section -->
        <section id="about" class="section">
            <div class="about-header">
                <div class="photo-container">
                    <img src="photo.jpg" alt="Kaiyang Zhou" class="profile-photo">
                </div>
                <div class="about-intro">
                    <h1>Kaiyang Zhou</h1>
                    <p class="title">Assistant Professor</p>
                    <p class="affiliation">Department of Computer Science<br>Hong Kong Baptist University</p>
                    <p class="email"><a id="email-link"></a></p>
                    <div class="social-links">
                        <a href="https://scholar.google.com/citations?user=gRIejugAAAAJ" target="_blank" class="social-link">
                            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor"><path d="M12 24a7 7 0 1 1 0-14 7 7 0 0 1 0 14zm0-24L0 9.5l4.838 3.94A8 8 0 0 1 12 9a8 8 0 0 1 7.162 4.44L24 9.5z"/></svg>
                            Google Scholar
                        </a>
                        <a href="https://github.com/KaiyangZhou" target="_blank" class="social-link">
                            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg>
                            GitHub
                        </a>
                        <a href="https://twitter.com/kaiyangzhou" target="_blank" class="social-link">
                            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor"><path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/></svg>
                            Twitter
                        </a>
                    </div>
                </div>
            </div>
            
            <div class="about-content">
                <p>
                    Prof. Kaiyang Zhou is an Assistant Professor in the Department of Computer Science at Hong Kong Baptist University.
                    His research interests include machine learning, computer vision, and multimodality.
                    He has published an edited book on <a href="https://link.springer.com/book/10.1007/978-3-031-94969-2" target="_blank">Large Vision-Language Models</a>
                    and more than 50 journal and conference papers in top-tier venues, including TPAMI, TIP, IJCV, CVPR, ICCV, ECCV, NeurIPS, ICLR, ICML, and AAAI.
                    His work has been cited over 19,000 times.
                    He is an associate editor of the International Journal of Computer Vision and regularly serves as an area chair
                    for prestigious conferences such as CVPR, ECCV, NeurIPS, ICML, and ICLR.
                    Before joining HKBU, he was a postdoc at
                    Nanyang Technological University, working with Prof. <a href="https://liuziwei7.github.io/" target="_blank">Ziwei Liu</a>
                    and Prof. <a href="https://www.mmlab-ntu.com/person/ccloy/" target="_blank">Chen Change Loy</a>. He received his PhD in Computer Science
                    from the University of Surrey, under the supervision of Prof. <a href="https://www.surrey.ac.uk/people/tao-xiang" target="_blank">Tao Xiang</a>.
                </p>
            </div>
        </section>

        <!-- News Section -->
        <section id="news" class="section">
            <h2>News</h2>
            <ul class="news-list">
                <li><span class="news-date">Dec 2025</span> Invited to serve as area chair of ECCV 2026.</li>
                <li><span class="news-date">Nov 2025</span> Invited to serve as area chair of ICML 2026.</li>
                <li><span class="news-date">Sep 2025</span> Our edited book <a href="https://link.springer.com/book/10.1007/978-3-031-94969-2" target="_blank">Large Vision-Language Models</a> is online.</li>
                <li><span class="news-date">Aug 2025</span> Invited to serve as area chair of ICLR 2026.</li>
                <li><span class="news-date">Aug 2025</span> Invited to serve as area chair of CVPR 2026.</li>
                <li><span class="news-date">Jul 2025</span> Invited to serve as area chair of AAAI 2026.</li>
            </ul>
        </section>

        <!-- Research Section -->
        <section id="research" class="section">
            <h2>Research</h2>

            <p class="section-intro">
                Generally interested in machine learning and computer vision, with a goal of building general-purpose
                intelligence that can see, reason, and act safely and reliably in the unpredictable world.
                Currently focusing on vision-language models, multimodality, agents, and embodied AI.
            </p>

            <div class="resource-bar">
                <a href="https://scholar.google.com/citations?hl=en&user=gRIejugAAAAJ&view_op=list_works&sortby=pubdate" target="_blank" class="resource-link">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor"><path d="M12 24a7 7 0 1 1 0-14 7 7 0 0 1 0 14zm0-24L0 9.5l4.838 3.94A8 8 0 0 1 12 9a8 8 0 0 1 7.162 4.44L24 9.5z"/></svg>
                    Full Publications
                </a>
                <a href="https://github.com/maifoundations" target="_blank" class="resource-link">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg>
                    Code & Projects
                </a>
                <a href="https://huggingface.co/maifoundations" target="_blank" class="resource-link">
                    <span class="hf-icon">ðŸ¤—</span>
                    Models & Datasets
                </a>
            </div>

            <h3>Recent Papers</h3>
            <p class="section-intro">
                The papers shown below give an overview of topics I am working on.
            </p>

            <ul class="publications-list">
                <li>
                    <span class="pub-title">Streaming Video Instruction Tuning</span>
                    <span class="pub-authors">Jiaer Xia, Peixian Chen, Mengdan Zhang, Xing Sun, <strong>Kaiyang Zhou</strong></span>
                    <span class="pub-venue"><em>arXiv, 2025</em></span>
                    <span class="pub-links">
                        [<a href="https://arxiv.org/pdf/2512.21334" target="_blank">paper</a>]
                        [<a href="https://github.com/maifoundations/Streamo" target="_blank">code</a>]
                        [<a href="https://huggingface.co/datasets/maifoundations/Streamo-Instruct-465K" target="_blank">dataset</a>]
                    </span>
                </li>
                <li>
                    <span class="pub-title">Learning to Think Fast and Slow for Visual Language Models</span>
                    <span class="pub-authors">Chenyu Lin, Cheng Chi, Jinlin Wu, Sharon Li, <strong>Kaiyang Zhou</strong></span>
                    <span class="pub-venue"><em>arXiv, 2025</em></span>
                    <span class="pub-links">
                        [<a href="https://arxiv.org/pdf/2511.16670" target="_blank">paper</a>]
                        [<a href="https://github.com/maifoundations/DualMindVLM" target="_blank">code</a>]
                        [<a href="https://huggingface.co/maifoundations/DualMindVLM" target="_blank">model</a>]
                    </span>
                </li>
                <li>
                    <span class="pub-title">Visionary-R1: Mitigating Shortcuts in Visual Reasoning with Reinforcement Learning</span>
                    <span class="pub-authors">Jiaer Xia, Yuhang Zang, Peng Gao, Sharon Li, <strong>Kaiyang Zhou</strong></span>
                    <span class="pub-venue"><em>arXiv, 2025</em></span>
                    <span class="pub-links">
                        [<a href="https://arxiv.org/pdf/2505.14677" target="_blank">paper</a>]
                        [<a href="https://github.com/maifoundations/Visionary-R1" target="_blank">code</a>]
                        [<a href="https://huggingface.co/maifoundations/Visionary-R1" target="_blank">model</a>]
                    </span>
                </li>
                <li>
                    <span class="pub-title">Fine-tuning Quantized Neural Networks with Zeroth-order Optimization</span>
                    <span class="pub-authors">Sifeng Shang, Jiayi Zhou, Chenyu Lin, Minxian Li, <strong>Kaiyang Zhou</strong></span>
                    <span class="pub-venue"><em>arXiv, 2025</em></span>
                    <span class="pub-links">
                        [<a href="https://arxiv.org/pdf/2505.13430" target="_blank">paper</a>]
                        [<a href="https://github.com/maifoundations/QZO" target="_blank">code</a>]
                    </span>
                </li>
                <li>
                    <span class="pub-title">Measuring Epistemic Humility in Multimodal Large Language Models</span>
                    <span class="pub-authors">Bingkui Tong, Jiaer Xia, Sifeng Shang, <strong>Kaiyang Zhou</strong></span>
                    <span class="pub-venue"><em>arXiv, 2025</em></span>
                    <span class="pub-links">
                        [<a href="https://arxiv.org/pdf/2509.09658" target="_blank">paper</a>]
                        [<a href="https://github.com/maifoundations/HumbleBench" target="_blank">code</a>]
                        [<a href="https://huggingface.co/datasets/maifoundations/HumbleBench" target="_blank">dataset</a>]
                    </span>
                </li>
                <li>
                    <span class="pub-title">Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding</span>
                    <span class="pub-authors">Bingkui Tong, Jiaer Xia, <strong>Kaiyang Zhou</strong></span>
                    <span class="pub-venue"><em>NeurIPS Workshop on Multimodal Algorithmic Reasoning, 2025</em></span>
                    <span class="pub-links">
                        [<a href="https://www.arxiv.org/pdf/2509.25177" target="_blank">paper</a>]
                        [<a href="https://github.com/maifoundations/LayerCD" target="_blank">code</a>]
                    </span>
                </li>
                <li>
                    <span class="pub-title">Bootstrapping Grounded Chain-of-Thought in Multimodal LLMs for Data-Efficient Model Adaptation</span>
                    <span class="pub-authors">Jiaer Xia, Bingkui Tong, Yuhang Zang, Rui Shao, <strong>Kaiyang Zhou</strong></span>
                    <span class="pub-venue"><em>ICCV (<strong>Highlight</strong>), 2025</em></span>
                    <span class="pub-links">
                        [<a href="https://arxiv.org/pdf/2507.02859" target="_blank">paper</a>]
                        [<a href="https://github.com/maifoundations/GCoT" target="_blank">code</a>]
                    </span>
                </li>
            </ul>

            <h3>Selected Publications</h3>

            <ul class="publications-list">
                <li>
                    <span class="pub-title">Conditional Prompt Learning for Vision-Language Models</span>
                    <span class="pub-authors"><strong>Kaiyang Zhou</strong>, Jingkang Yang, Chen Change Loy, Ziwei Liu</span>
                    <span class="pub-venue"><em>CVPR, 2022</em></span>
                    <span class="pub-links">
                        [<a href="https://arxiv.org/pdf/2203.05557" target="_blank">paper</a>]
                        [<a href="https://github.com/KaiyangZhou/CoOp" target="_blank">code</a>]
                    </span>
                </li>
                <li>
                    <span class="pub-title">Learning to Prompt for Vision-Language Models</span>
                    <span class="pub-authors"><strong>Kaiyang Zhou</strong>, Jingkang Yang, Chen Change Loy, Ziwei Liu</span>
                    <span class="pub-venue"><em>IJCV, 2022</em></span>
                    <span class="pub-links">
                        [<a href="https://arxiv.org/pdf/2109.01134" target="_blank">paper</a>]
                        [<a href="https://github.com/KaiyangZhou/CoOp" target="_blank">code</a>]
                    </span>
                </li>
                <li>
                    <span class="pub-title">Domain Generalization with MixStyle</span>
                    <span class="pub-authors"><strong>Kaiyang Zhou</strong>, Yongxin Yang, Yu Qiao, Tao Xiang</span>
                    <span class="pub-venue"><em>ICLR, 2021</em></span>
                    <span class="pub-links">
                        [<a href="https://arxiv.org/pdf/2104.02008" target="_blank">paper</a>]
                        [<a href="https://github.com/KaiyangZhou/mixstyle-release" target="_blank">code</a>]
                    </span>
                </li>
                <li>
                    <span class="pub-title">Omni-Scale Feature Learning for Person Re-Identification</span>
                    <span class="pub-authors"><strong>Kaiyang Zhou</strong>, Yongxin Yang, Andrea Cavallaro, Tao Xiang</span>
                    <span class="pub-venue"><em>ICCV, 2019</em></span>
                    <span class="pub-links">
                        [<a href="https://arxiv.org/pdf/1905.00953" target="_blank">paper</a>]
                        [<a href="https://github.com/KaiyangZhou/deep-person-reid" target="_blank">code</a>]
                        [<a href="https://huggingface.co/kaiyangzhou/osnet" target="_blank">model</a>]
                    </span>
                </li>
                <li>
                    <span class="pub-title">Deep Reinforcement Learning for Unsupervised Video Summarization with Diversity-Representativeness Reward</span>
                    <span class="pub-authors"><strong>Kaiyang Zhou</strong>, Yu Qiao, Tao Xiang</span>
                    <span class="pub-venue"><em>AAAI, 2018</em></span>
                    <span class="pub-links">
                        [<a href="https://arxiv.org/pdf/1801.00054" target="_blank">paper</a>]
                        [<a href="https://github.com/KaiyangZhou/pytorch-vsumm-reinforce" target="_blank">code</a>]
                    </span>
                </li>
            </ul>
        </section>

        <!-- Team Section -->
        <section id="team" class="section">
            <h2>Team</h2>
            
            <div class="team-intro">
                <button id="join-us-btn" class="more-info-btn">
                    <span class="btn-text">Join Us</span>
                    <svg class="btn-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2.5" stroke-linecap="round" stroke-linejoin="round">
                        <polyline points="6 9 12 15 18 9"></polyline>
                    </svg>
                </button>
            </div>
            
            <div id="join-us-content" class="detailed-bio">
                <p>
                    I am recruiting motivated PhD students/research assistants interested in LLM/VLM/Agents/Robotics.
                    Ideal candidates should have a strong background in ML/CV/NLP, solid coding skills, and prior research experience.
                    If you are passionate about doing cutting-edge AI research with us,
                    please send me an email with your CV, transcripts, relevant publications or projects, and research statement (if any).
                </p>
            </div>
            
            <h3>PhD Students</h3>
            <ul class="team-list">
                <li><a href="https://jiaerxia.github.io/" target="_blank">Jiaer Xia</a> (2024 - Present)</li>
                <li><a href="https://sifengshang.github.io/" target="_blank">Sifeng Shang</a> (2024 - Present)</li>
                <li><a href="https://zhoujiayi003.github.io/" target="_blank">Jiayi Zhou</a> (2025 - Present)</li>
                <li><a href="https://chenyulin-craig.github.io/" target="_blank">Chenyu Lin</a> (2025 - Present)</li>
            </ul>

            <h3>Research Assistants</h3>
            <ul class="team-list">
                <li><a href="https://panlinchao.github.io/" target="_blank">Linchao Pan</a> (2025 - Present)</li>
                <li><a href="https://haichenhe.github.io/" target="_blank">Haichen He</a> (2025 - Present)</li>
            </ul>

            <h3>Alumni</h3>
            <ul class="team-list">
                <li><a href="https://yutchina.github.io/" target="_blank">Yu Tong</a> (RA 2025)</li>
                <li><a href="https://tbbbk.github.io/" target="_blank">Bingkui Tong</a> (RA 2024-25, now PhD at MBZUAI)</li>
            </ul>
        </section>

        <!-- Teaching Section -->
        <section id="teaching" class="section">
            <h2>Teaching</h2>
            
            <div class="teaching-list">
                <div class="course">
                    <span class="course-code">COMP 7040</span>
                    <span class="course-name">Advanced Topics in Computer Vision and Pattern Recognition</span>
                </div>
                <div class="course">
                    <span class="course-code">COMP 3076</span>
                    <span class="course-name">AI and Generative Arts</span>
                </div>
                <div class="course">
                    <span class="course-code">COMP 7065</span>
                    <span class="course-name">Innovative Laboratory</span>
                </div>
            </div>
        </section>

        <!-- Services Section -->
        <section id="services" class="section">
            <h2>Services</h2>
            
            <ul class="services-list">
                <li>Associate Editor, <em>International Journal of Computer Vision (IJCV)</em> (2023 - Present)</li>
                <li>Guest Editor, <em>IJCV Special Issue on Visual Domain Generalization in Real-World Applications</em> (2024)</li>
                <li>Guest Editor, <a href="https://link.springer.com/article/10.1007/s11263-023-01941-4" target="_blank">IJCV Special Issue on The Promises and Dangers of Large Vision Models</a> (2023)</li>
                <li>Area Chair, <em>International Conference on Machine Learning (ICML)</em> (2025, 2026)</li>
                <li>Area Chair, <em>International Conference on Learning Representations (ICLR)</em> (2025, 2026)</li>
                <li>Area Chair, <em>Neural Information Processing Systems (NeurIPS)</em> (2024, 2025)</li>
                <li>Area Chair, <em>Computer Vision and Pattern Recognition (CVPR)</em> (2024, 2026)</li>
                <li>Area Chair, <em>European Conference on Computer Vision (ECCV)</em> (2024, 2026)</li>
                <li>Area Chair, <em>AAAI Conference on Artificial Intelligence (AAAI)</em> (2023 - 2026)</li>
                <li>Area Chair, <em>British Machine Vision Conference (BMVC)</em> (2022, 2024)</li>
                <li>Organizer, <a href="https://cvpr25workshop.m-haris-khan.com/" target="_blank">CVPR 2025 Workshop on Domain Generalization</a></li>
                <li>Organizer, <a href="https://green-fomo.github.io/ECCV2024/index.html" target="_blank">ECCV 2024 Workshop on Green Foundation Models</a></li>
                <li>Organizer, <a href="https://prompting-in-vision.github.io/index_cvpr24.html" target="_blank">CVPR 2024 Workshop on Prompting in Vision </a></li>
                <li>Organizer, <a href="https://prompting-in-vision.github.io/index_cvpr23.html" target="_blank">CVPR 2023 Tutorial on Prompting in Vision</a></li>
                <li>Organizer, <a href="https://domaingen.github.io" target="_blank">ICLR 2023 Workshop on What Do We Need for Successful Domain Generalization</a></li>
                <li>Organizer, <a href="https://theaitalks.org" target="_blank">The AI Talks</a></li>
            </ul>
        </section>

        <!-- Awards Section -->
        <section id="awards" class="section">
            <h2>Awards</h2>
            
            <ul class="awards-list">
                <li><span class="award-year">2025</span> <a href="https://github.com/KaiyangZhou/CoOp" target="_blank">CoOp</a> received WAIC Youth Outstanding Paper Nomination Award </li>
                <li><span class="award-year">2024</span> HKBU Research Excellence Paper Award </li>
                <li><span class="award-year">2023</span> Worldâ€™s Top 2% Scientists </li>
                <li><span class="award-year">2022</span> <a href="https://github.com/KaiyangZhou/CoOp" target="_blank">CoCoOp</a> received <a href="https://www.zeta-alpha.com/post/must-read-the-100-most-cited-ai-papers-in-2022" target="_blank">Top-100 Most Cited AI Paper in 2022</a> </li>
                <li><span class="award-year">2022</span> ECCV 2022 Outstanding Reviewer </li>
                <li><span class="award-year">2021</span> ICCV 2021 Outstanding Reviewer </li>
                <li><span class="award-year">2021</span> AAAI 2021 Top 25% of Program Committee Members</li>
            </ul>
        </section>
    </main>

    <footer class="footer">
        <p>Last update: Jan 2026</p>
    </footer>

    <!-- Back to Top Button -->
    <button id="back-to-top" class="back-to-top" aria-label="Back to top">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2.5" stroke-linecap="round" stroke-linejoin="round">
            <polyline points="18 15 12 9 6 15"></polyline>
        </svg>
    </button>

    <script>
        // Email obfuscation to prevent spam bots
        (function() {
            const u = 'kyzhou';
            const d = 'hkbu.edu.hk';
            const el = document.getElementById('email-link');
            if (el) {
                el.href = 'mail' + 'to:' + u + '@' + d;
                el.textContent = u + '@' + d;
            }
        })();

        // Back to top button
        (function() {
            const btn = document.getElementById('back-to-top');
            
            window.addEventListener('scroll', function() {
                if (window.scrollY > 300) {
                    btn.classList.add('visible');
                } else {
                    btn.classList.remove('visible');
                }
            });

            btn.addEventListener('click', function() {
                window.scrollTo({ top: 0, behavior: 'smooth' });
            });
        })();

        // Join Us toggle
        (function() {
            const btn = document.getElementById('join-us-btn');
            const joinUsContent = document.getElementById('join-us-content');
            
            if (btn && joinUsContent) {
                btn.addEventListener('click', function() {
                    const isExpanded = joinUsContent.classList.toggle('expanded');
                    btn.classList.toggle('expanded', isExpanded);
                    btn.querySelector('.btn-text').textContent = isExpanded ? 'Hide Info' : 'Join Us';
                });
            }
        })();
    </script>
</body>
</html>

